[env]
time_limit = 50
time_step = 0.25
val_size = 100
test_size = 500
randomize_attributes = false
discretization = 84
adversary_action_scaling = 0.1
adversary_state_scaling = 0.01
# Add gaussian noise to the observed states
add_gaussian_noise_state = false
gaussian_noise_state_stddev = 0.01
# Add gaussian noise to the actions
add_gaussian_noise_action = false
gaussian_noise_action_stddev = 0.1

[reward]
success_reward = 10
#reward for reaching goal
collision_penalty = -0.25
# penalty for colliding with humans
discomfort_dist = 0
# distance where robot is too close to human (adopted from CrowdNav)
discomfort_penalty_factor = 0
# penalty for being within discomfort_dist of human (adopted from CrowdNav)
edge_discomfort_dist = 0
# if robot is within edge_discomfort_dist of the edge of the accessible space, edge_penalty applied
edge_penalty = -.1
closer_goal = 0
# reward for moving closer to the goal in a step


[sim]
train_val_sim = circle_crossing
test_sim = circle_crossing
square_width = 10
# width of square for 'square_crossing' behaviour (adopted from CrowdNav)
circle_radius = 3
# radius of circle for 'circle_crossing' behaviour (adopted from CrowdNav)
accessible_space = 4
# accessible space robot can navigate through is x \in [-accessible_space, accessible_space], y \in [-accessible_space, accessible_space]
goal_region = 3
# goals can be generated within x \in [-goal_region, goal_region], y \in [-goal_region, goal_region]
randomize_goals = True
# if true, then goals will be randomly generated
update_goals = false
# if true, then a new goal will be generated every time a robot reaches it. else, done when goal is reached
human_num = 1

[humans]
visible = true
policy = orca
radius = 0.3
# WARNING: This needs to be scaled appropriately by the timestep
v_pref = 0.15
sensor = coordinates

[robot]
visible = true
policy = none
radius = 0.3
v_pref = 0.2
sensor = coordinates

[train_details]
num_stacked_frames = 2
train_on_images = false
show_images = false

[ma_train_details]
perturb_actions = false
perturb_state = true

[transfer]
# If first_step the colors will be swapped every reset, if every_step the colors will be swapped every call to step
change_colors_mode = false
# If true, the actions that are actually passed to the robot will be slightly lower than the actions output by the policy
friction = false
# This is a scaling factor on how much less the actions are if friction is true. Check agent.py for more details.
friction_coef = 0
# If true, then human default goal will be robot's next location. Else, default human behaviour (circle_crossing)
chase_robot = false
# If false, then goals can be generated anywhere within accessible_space. Else they will only be generated in the specified goal_region
restrict_goal_region = false
