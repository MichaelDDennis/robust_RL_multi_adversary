[env]
time_limit = 50
time_step = 0.25
val_size = 100
test_size = 500
randomize_attributes = false
discretization = 84


[reward]
success_reward = 10
#reward for reaching goal
collision_penalty = -0.25
# penalty for colliding with humans
discomfort_dist = 0
discomfort_penalty_factor = 0
edge_discomfort_dist = 0.3
# if robot is within edge_discomfort_dist of the edge of the accessible space, edge_penalty applied
edge_penalty = -.1
closer_goal = .1
# reward for moving closer to the goal in a step


[sim]
train_val_sim = circle_crossing
test_sim = circle_crossing
square_width = 10
circle_radius = 3
accessible_space = 4
# accessible space robot can navigate through is x \in [-accessible_space, accessible_space], y \in [-accessible_space, accessible_space]
goal_region = 3
# goals can be generated within x \in [-goal_region, goal_region], y \in [-goal_region, goal_region]
randomize_goals = true
# if true, then goals will be randomly generated
update_goals = true
# if true, then a new goal will be generated every time a robot reaches it. else, done when goal is reached
human_num = 1


[humans]
visible = true
policy = orca
radius = 0.3
v_pref = 0.5
sensor = coordinates
chase_robot = true
# if true, then human default goal will be robot's next location. Else, default human behaviour (circle_crossing)


[robot]
visible = true
policy = none
radius = 0.3
v_pref = 1
sensor = coordinates


[train_details]
num_stacked_frames = 2