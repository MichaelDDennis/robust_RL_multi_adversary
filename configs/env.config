[env]
time_limit = 50
time_step = 0.25
val_size = 100
test_size = 500
randomize_attributes = false
discretization = 162


[reward]
success_reward = 10
collision_penalty = -0.25
discomfort_dist = 0
discomfort_penalty_factor = 0
edge_discomfort_dist = 0.1
edge_penalty = -0.01
closer_goal = 0.1


[sim]
train_val_sim = circle_crossing
test_sim = circle_crossing
square_width = 10
circle_radius = 3
accessible_space = 5
goal_region = 3
human_num = 1


[humans]
visible = true
policy = orca
radius = 0.3
v_pref = 0.5
sensor = coordinates


[robot]
visible = true
policy = none
radius = 0.3
v_pref = 1
sensor = coordinates


[train_details]
# This is how many timesteps worth of frames are input to the policy
num_stacked_frames = 2
# This is the x and y limits of the grid size
grid_limit = 6


[transfer]
# If first_step the colors will be swapped every reset, if every_step the colors will be swapped every call to step
change_colors_mode = no_change
# If true, the actions that are actually passed to the robot will be slightly lower than the actions output by the policy
friction = false
# This is a scaling factor on how much less the actions are if friction is true. Check agent.py for more details.
friction_coef = 0.2
